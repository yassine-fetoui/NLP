{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1YbXKsVIGCIyj6LW1CSS6Prbdc7gG7-Nv",
      "authorship_tag": "ABX9TyP9rPHQ1Q9wl1HlJerYZ4d2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yassine-fetoui/NLP/blob/main/Sentiment_Analysis_RNN_Graph_V1/Sentiment_Analysis_RNN_Graph_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUx_Tbo-zhPu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/reviews.txt', 'r') as f:\n",
        "  reviews=f.read()\n",
        "with open('/content/drive/MyDrive/labels.txt', 'r') as f:\n",
        "  labels=f.read()"
      ],
      "metadata": {
        "id": "BvARJVii1RAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(reviews)"
      ],
      "metadata": {
        "id": "zuehhWfi4rT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviews[:2000].count('\\n'))"
      ],
      "metadata": {
        "id": "UazP-PKz4QDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviews[:2000])"
      ],
      "metadata": {
        "id": "JCm8d3h31dtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "all_text=''.join([c for c in reviews if c not in punctuation])\n",
        "reviews = all_text.split('\\n')\n",
        "\n",
        "all_text = ' '.join(reviews)\n",
        "words = set(all_text.split())"
      ],
      "metadata": {
        "id": "kCp1NbZ00l9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in words:\n",
        "  print(i)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pJIEVchw3CUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}"
      ],
      "metadata": {
        "id": "ucClcUof3gRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_to_int['rockwell'])"
      ],
      "metadata": {
        "id": "9XpPAA-76Ffe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(counts.get())\n",
        "\"\"\"key=counts.get is a key function used by the sorted function to determine the sort order.\n",
        " Specifically, it tells the sorted function to use the counts (frequencies)\n",
        "  of the words as the key for sorting.\"\"\""
      ],
      "metadata": {
        "id": "cTqlDrI77N8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[:20]"
      ],
      "metadata": {
        "id": "ffhX1Qbi6aGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_ints  = []\n",
        "for review in reviews:\n",
        "  reviews_ints.append([vocab_to_int[word] for word in review.split()])\n"
      ],
      "metadata": {
        "id": "5apY6hu1737N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_to_int[reviews[0].split()[0]]"
      ],
      "metadata": {
        "id": "Etf55sRdOE5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[0].split()[0]"
      ],
      "metadata": {
        "id": "OX22sAeFUj85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_to_int[\"bromwell\"]"
      ],
      "metadata": {
        "id": "RwvF2vluUmh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_ints[0]"
      ],
      "metadata": {
        "id": "_maLqd0jT-fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I4B5FAm6U61O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoding the Labels**"
      ],
      "metadata": {
        "id": "uKyGz1n8U-oJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ],
      "metadata": {
        "id": "rhZscYkiUu0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels"
      ],
      "metadata": {
        "id": "QIoVJqD9U4x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_len = Counter([len(x) for x in reviews_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_len[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_len)))"
      ],
      "metadata": {
        "id": "mDkcKGm8VNZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out that review with 0 length\n",
        "reviews_ints = [each for each in reviews_ints if len(each) > 0]"
      ],
      "metadata": {
        "id": "D8J7aawnVeb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 200\n",
        "features = np.zeros((len(reviews_ints), seq_len), dtype=int)"
      ],
      "metadata": {
        "id": "4a234MQWWDJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape"
      ],
      "metadata": {
        "id": "epI-y28tSffV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(reviews_ints[2])"
      ],
      "metadata": {
        "id": "FIIyOOa6S3mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(reviews_ints[2][:seq_len])"
      ],
      "metadata": {
        "id": "ZiQp-jrrTnIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,row in enumerate(reviews_ints):\n",
        "  features[i, -len(row):] = np.array(row)[:seq_len]"
      ],
      "metadata": {
        "id": "xkd_d7baSlKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(features[0,:] )"
      ],
      "metadata": {
        "id": "yJx7obaKTLLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training, Validation, Test**"
      ],
      "metadata": {
        "id": "BTNpyQu4UxKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "split_idx = int(len(features)*0.8)\n",
        "train_x, val_x = features[:split_idx], features[split_idx:]\n",
        "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(val_x)*0.5)\n",
        "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
        "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
        "\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
        "## split data into training, validation, and test data (features and labels, x and y)"
      ],
      "metadata": {
        "id": "KHTowgH6UJ2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_size = 256\n",
        "lstm_layers = 1\n",
        "batch_size = 500\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "knxiD2GTVd6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = len(vocab)\n",
        "\n",
        "# Create the graph object\n",
        "graph = tf.Graph()\n",
        "# Add nodes to the graph\n",
        "with graph.as_default():\n",
        "    inputs_ = tf.compat.v1.placeholder(tf.int32, [None, None], name='inputs')\n",
        "    labels_ = tf.compat.v1.placeholder(tf.int32, [None, None], name='labels')\n",
        "    keep_prob =  tf.compat.v1.placeholder(tf.float32, name='keep_prob')"
      ],
      "metadata": {
        "id": "4BMFuJ5NWbKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_.shape"
      ],
      "metadata": {
        "id": "rx6JbhmYgz2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WK_fwFI1gyG6"
      }
    },
    {
      "source": [
        "# Size of the embedding vectors (number of units in the embedding layer)\n",
        "embed_size = 300\n",
        "\n",
        "with graph.as_default():\n",
        "    embedding = tf.Variable(tf.random.uniform((n_words, embed_size), -1, 1))\n",
        "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "AJQV1023WpX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with graph.as_default():\n",
        "    # Your basic LSTM cell\n",
        "    lstm =tf.compat.v1.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
        "\n",
        "    # Add dropout to the cell\n",
        "    drop = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
        "\n",
        "    # Stack up multiple LSTM layers, for deep learning\n",
        "    cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([drop] * lstm_layers)\n",
        "\n",
        "    # Getting an initial state of all zeros\n",
        "    initial_state = cell.zero_state(batch_size, tf.float32)"
      ],
      "metadata": {
        "id": "uhbhoDSLhxOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with graph.as_default():\n",
        "    outputs, final_state = tf.compat.v1.nn.dynamic_rnn(cell, embed,\n",
        "                                             initial_state=initial_state)"
      ],
      "metadata": {
        "id": "bYLyNRAXin7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with graph.as_default():\n",
        "    predictions = tf.keras.layers.Dense(1, activation='sigmoid')(outputs[:, -1])    #predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
        "\n",
        "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
        "\n",
        "    optimizer=  tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    train_step=optimizer.minimize(cost, var_list=[embedding] +  tf.compat.v1.trainable_variables()) # Pass the cost function and variables to minimize\n",
        "\n"
      ],
      "metadata": {
        "id": "KlzI-AHIl4Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "with graph.as_default():\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "\n",
        "with tf.compat.v1.Session(graph=graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    iteration = 1\n",
        "    for e in range(epochs):\n",
        "        state = sess.run(initial_state)\n",
        "\n",
        "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
        "            feed = {inputs_: x,\n",
        "                    labels_: y[:, None],\n",
        "                    keep_prob: 0.5,\n",
        "                    initial_state: state}\n",
        "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
        "\n",
        "            if iteration%5==0:\n",
        "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
        "                      \"Iteration: {}\".format(iteration),\n",
        "                      \"Train loss: {:.3f}\".format(loss))\n",
        "\n",
        "            if iteration%25==0:\n",
        "                val_acc = []\n",
        "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
        "                for x, y in get_batches(val_x, val_y, batch_size):\n",
        "                    feed = {inputs_: x,\n",
        "                            labels_: y[:, None],\n",
        "                            keep_prob: 1,\n",
        "                            initial_state: val_state}\n",
        "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
        "                    val_acc.append(batch_acc)\n",
        "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
        "            iteration +=1\n",
        "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
      ],
      "metadata": {
        "id": "a9M07ySiQD0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}