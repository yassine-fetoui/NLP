{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1hSoZotvICux1tD-n29vydvYxCqGAtT7L",
      "authorship_tag": "ABX9TyPbBZnWhVkLekOjt3epxfq9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTnC06mDG3N7"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import  numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path=\"/content/drive/MyDrive/eng-fra.txt\"\n",
        "with open(data_path,'r',encoding='utf_8') as f:\n",
        "  lines=f.read()\n",
        "lines"
      ],
      "metadata": {
        "id": "2oMltSqCOPnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lines(text):\n",
        "  sents = text.strip().split(\"\\n\")\n",
        "  sents = [i.split('\\t') for i in sents]\n",
        "  return sents"
      ],
      "metadata": {
        "id": "XPIZgiXnPdq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_eng = to_lines(lines)"
      ],
      "metadata": {
        "id": "N2TY3p0fz-OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_eng[:9]"
      ],
      "metadata": {
        "id": "eVbThdYS0Jak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_eng=np.array(fra_eng)\n",
        "fra_eng[:5]"
      ],
      "metadata": {
        "id": "EQUDdDXX5xr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_eng.shape\n"
      ],
      "metadata": {
        "id": "xT-YhNxGDDm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove puntuation\n",
        "fra_eng= fra_eng[:90000,:]"
      ],
      "metadata": {
        "id": "Ls8uDNLiEOku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_eng[:5]"
      ],
      "metadata": {
        "id": "RfTyzwdXEiTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_eng[:,0]=[s.translate(str.maketrans(\"\",\"\",string.punctuation)) for s in fra_eng[:,0]]\n",
        "fra_eng[:,1]=[s.translate(str.maketrans(\"\",\"\",string.punctuation)) for s in fra_eng[:,1]]\n",
        "\n",
        "\n",
        "fra_eng[:5]"
      ],
      "metadata": {
        "id": "rJGUifOmEp8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(fra_eng)) :\n",
        "  fra_eng[i,0]=fra_eng[i,0].lower()\n",
        "  fra_eng[i,1]=fra_eng[i,1].lower()\n",
        "\n"
      ],
      "metadata": {
        "id": "K7v-bzAyFJHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_eng"
      ],
      "metadata": {
        "id": "ue1kcBmYFeHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text to Sequence Conversion(word to index mapping)**\n",
        "\n",
        "\n",
        "1.   Convert sentences into numbers\n",
        "2.   Every sentence should be of same length\n",
        "\n"
      ],
      "metadata": {
        "id": "8QsR76mPFk8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(lines):\n",
        "  tokenizer=Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "5oYZc_OgGAF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = tokenization(fra_eng[:,0])\n",
        "eng_vocab_size= len(eng_tokenizer.word_index)\n",
        "\n",
        "eng_length=8\n",
        "print(\"English Vocabs size: %d\"% eng_vocab_size)"
      ],
      "metadata": {
        "id": "vLOWb7AJFjtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare french tokenizer\n",
        "\n",
        "fra_tokenizer = tokenization(fra_eng[:,1])\n",
        "fra_vocab_size= len(fra_tokenizer.word_index)\n",
        "\n",
        "fra_length=8\n",
        "print(\"French Vocabs size:%d\"% fra_vocab_size)"
      ],
      "metadata": {
        "id": "kI8BcaBrHZyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode and pad sequences, padding to a maximum sentence length as mentioned above\n",
        "def encode_sequences(tokenizer,length,lines):\n",
        "\n",
        "  seq=tokenizer.texts_to_sequences(lines)\n",
        "  seq=pad_sequences(seq,maxlen=length,padding='post') # same length\n",
        "  return seq"
      ],
      "metadata": {
        "id": "ZZ9Fc8F4L7Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to encode the sentences. We will encode French sentences as the input sequences and English sentences as the target sequences. This had to be done for both the train and test datasets."
      ],
      "metadata": {
        "id": "-5C3u8T0W2KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection  import train_test_split\n",
        "train, test= train_test_split(fra_eng,test_size=2,random_state=12)"
      ],
      "metadata": {
        "id": "rnzYHCBAMrbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare training data\n",
        "TrainX= encode_sequences(fra_tokenizer,fra_length,train[:,1])\n",
        "TrainY= encode_sequences(eng_tokenizer,eng_length,train[:,0])"
      ],
      "metadata": {
        "id": "nc26f8pyYb--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare validation data\n",
        "testX = encode_sequences(fra_tokenizer,fra_length,test[:,1])\n",
        "testY = encode_sequences(eng_tokenizer,eng_length,test[:,0])"
      ],
      "metadata": {
        "id": "cwKnLrBpYz-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Seq2seq model architecture***"
      ],
      "metadata": {
        "id": "PNz9RCgRaWEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Model_seq2seq(in_vocab,out_vocab,in_timesteps,out_timesteps,units):\n",
        "  model=Sequential()\n",
        "  model.add(Embedding(in_vocab,units,input_length=in_timesteps,mask_zero=True)) # encoder\n",
        "  model.add(LSTM(units))  # encoder\n",
        "  model.add(RepeatVector(out_timesteps))\n",
        "  model.add(LSTM(units,return_sequences=True)) # decoder\n",
        "  model.add(Dense(out_vocab,activation='softmax')) # decoder\n",
        "  return model"
      ],
      "metadata": {
        "id": "XvFJ-v0yaf5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model_seq2seq(fra_vocab_size,eng_vocab_size,fra_length,eng_length,512)\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms,loss=\"sparse_categorical_crossentropy\")"
      ],
      "metadata": {
        "id": "6OqPcQ1leI3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "T1RgI-_qgwYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train the model"
      ],
      "metadata": {
        "id": "n5mB5lDVfJeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history= model.fit(TrainX,TrainY.reshape(TrainX.shape[0],TrainY.shape[1],1),epochs=15,batch_size=512,validation_split=0.2)"
      ],
      "metadata": {
        "id": "huSI8ii_fEnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
      ],
      "metadata": {
        "id": "RnMS4mUWhP2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#integer ------> words\n",
        "def get_words(n,tokenizer):\n",
        "  for word, index in tokenizer.word_index.item():\n",
        "    if index==n:\n",
        "      return word\n",
        "  return None\n"
      ],
      "metadata": {
        "id": "2kGo1KYMhusZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***convert predictions into sentences(English): ***"
      ],
      "metadata": {
        "id": "zB1b3rj-iQWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_text=[]\n",
        "for i in preds :\n",
        "  temp=[]\n",
        "  for j in range(len(i)):\n",
        "    t=get_words(i[j],eng_tokenizer)\n",
        "    if j > 0 :\n",
        "      if(t==get_words(i[j-1],eng_tokenizer)) or (t==None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t)\n",
        "  pred_text.append(' '.join(temp))\n"
      ],
      "metadata": {
        "id": "gYo7cZs9ibRq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
